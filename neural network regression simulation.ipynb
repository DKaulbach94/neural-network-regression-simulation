{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.12.0\n",
      "Numpy version: 1.15.4\n"
     ]
    }
   ],
   "source": [
    "print (\"TensorFlow version: \" + tf.__version__)\n",
    "print (\"Numpy version: \" + np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nfeatures ist die Dimension des Inputs.\n",
    "nfeatures = 3\n",
    "\n",
    "#Durch den festgelegten seed werden immer die gleichen Daten erzeugt.\n",
    "#Wählt man als batch_size einmal die Zahl n und ein anderes mal die Zahl n+1,\n",
    "#So gleichen sich jeweils die ersten n Daten.\n",
    "def create_data(batch_size, stddev):\n",
    "    #Parameter für die Dimension des Inputs:\n",
    "    \n",
    "    #Hier werden die Daten generiert:\n",
    "    #Input:\n",
    "    input_learn = tf.random_uniform([batch_size, nfeatures], minval=0,\n",
    "                                    maxval=100, seed=3140, name=\"input\")\n",
    "\n",
    "    #Output:\n",
    "    factors = tf.constant([[2], [-5], [4]], dtype=\"float32\")\n",
    "    norm = tf.random_normal([batch_size, 1], mean=0, stddev=stddev, seed=8235)\n",
    "    output_learn3 = tf.linalg.matmul(input_learn, factors)\n",
    "    output_learn2 = tf.math.add(output_learn3,\n",
    "                                tf.constant(30.0, shape=[batch_size, 1]))\n",
    "    output_learn = tf.math.add(output_learn2, norm, name=\"output\")\n",
    "\n",
    "    \n",
    "    #Die Daten werden ausgeführt:\n",
    "    sess = tf.InteractiveSession()\n",
    "    input_learn_run = sess.run(input_learn)\n",
    "    output_learn_run = sess.run(output_learn,\n",
    "                              feed_dict={input_learn: input_learn_run})\n",
    "    sess.close()\n",
    "    \n",
    "    return input_learn_run, output_learn_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholder für Input und Output werden definiert:\n",
    "x = tf.placeholder(tf.float32, [None, nfeatures])\n",
    "y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier werden alle Parameter definiert:\n",
    "tf.set_random_seed(1735)\n",
    "\n",
    "# Gewichte zwischen dem Input und hidden-layer nr. 1 mit 10 Knoten:\n",
    "W1 = tf.Variable(tf.random_uniform([nfeatures, 10], minval=-0.6794,\n",
    "                                   maxval=0.6794), name='W1')\n",
    "#Bias von hidden-layer nr. 1:\n",
    "b1 = tf.Variable(tf.random_normal([10], mean=0, stddev=0), name='b1')\n",
    "# Gewichte zwischen hidden-layer nr.1 und hidden-layer nr. 2 mit 50 Knoten:\n",
    "W2 = tf.Variable(tf.random_uniform([10, 10], minval=-0.5477,\n",
    "                                   maxval=0.5477), name='W2')\n",
    "#Bias von hidden-layer nr. 2:\n",
    "b2 = tf.Variable(tf.random_normal([10], mean=0, stddev=0), name='b2')\n",
    "# Gewichte zwischen hidden-layer nr.2 und hidden-layer nr. 3 mit 50 Knoten:\n",
    "W3 = tf.Variable(tf.random_uniform([10, 10], minval=-0.5477,\n",
    "                                   maxval=0.5477), name='W3')\n",
    "#Bias von hidden-layer nr. 3:\n",
    "b3 = tf.Variable(tf.random_normal([10], mean=0, stddev=0), name='b3')\n",
    "# Gewichte zwischen hidden-layer nr.3 und dem Output:\n",
    "W4 = tf.Variable(tf.random_uniform([10, 1], minval=-0.7385,\n",
    "                                   maxval=0.7385), name='W4')\n",
    "#Bias des Outputs:\n",
    "b4 = tf.Variable(tf.random_normal([1], mean=0, stddev=0), name='b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier werden alle Zustandsvariablen berechnet:\n",
    "# Berechne die Werte von hidden-layer nr.1:\n",
    "hidden_1_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_1_out = tf.nn.relu(hidden_1_out)\n",
    "# Berechne die Werte von hidden-layer nr.2:\n",
    "hidden_2_out = tf.add(tf.matmul(hidden_1_out, W2), b2)\n",
    "hidden_2_out = tf.nn.relu(hidden_2_out)\n",
    "# Berechne die Werte von hidden-layer nr.3:\n",
    "hidden_3_out = tf.add(tf.matmul(hidden_2_out, W3), b3)\n",
    "hidden_3_out = tf.nn.relu(hidden_3_out)\n",
    "# Berechne den Output: \n",
    "y_pred = tf.add(tf.matmul(hidden_3_out, W4), b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Das Format von loss_minibatch ist für die Anwandung in der\n",
    "#Funktion train_minibatch() zugeschnitten.\n",
    "loss_minibatch = tf.reduce_mean(tf.squared_difference(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier wird das Risiko empirisch bestimmt:\n",
    "x_test = tf.placeholder(tf.float32, [None, nfeatures])\n",
    "y_test = tf.placeholder(tf.float32, [None, 1])\n",
    "# Berechne die Werte von hidden-layer nr.1:\n",
    "hidden_1_out_test = tf.add(tf.matmul(x_test, W1), b1)\n",
    "hidden_1_out_test = tf.nn.relu(hidden_1_out_test)\n",
    "# Berechne die Werte von hidden-layer nr.2:\n",
    "hidden_2_out_test = tf.add(tf.matmul(hidden_1_out_test, W2), b2)\n",
    "hidden_2_out_test = tf.nn.relu(hidden_2_out_test)\n",
    "# Berechne die Werte von hidden-layer nr.3:\n",
    "hidden_3_out_test = tf.add(tf.matmul(hidden_2_out_test, W3), b3)\n",
    "hidden_3_out_test = tf.nn.relu(hidden_3_out_test)\n",
    "# Berechne den Output:\n",
    "y_pred_test = tf.add(tf.matmul(hidden_3_out_test, W4), b4)\n",
    "\n",
    "#Berechne das empirische Risiko:\n",
    "risk = tf.reduce_mean(tf.squared_difference(y_test, y_pred_test))\n",
    "\n",
    "def estimate_risk(test_size, sess, j, stddev):\n",
    "    #j wird als seed verwendet, damit bei verschiedenen estimate_risk\n",
    "    #verschiedene Testdaten erzeugt werden\n",
    "    #Testdaten:\n",
    "    #Input:\n",
    "    input_test = tf.random_uniform([test_size, nfeatures], minval=0,\n",
    "                                   maxval=100, name=\"input_test\", seed=j)\n",
    "\n",
    "    #Output:\n",
    "    factors_test = tf.constant([[2], [-5], [4]], dtype=\"float32\")\n",
    "    norm_test = tf.random_normal([test_size, 1], mean=0, stddev=stddev,\n",
    "                                 seed=2*j)\n",
    "    output_test3 = tf.linalg.matmul(input_test, factors_test)\n",
    "    output_test2 = tf.math.add(output_test3,\n",
    "                               tf.constant(30.0, shape=[test_size, 1]))\n",
    "    output_test = tf.math.add(output_test2, norm_test, name=\"output_test\")\n",
    "\n",
    "    input_test_run = sess.run(input_test)\n",
    "    output_test_run = sess.run(output_test,\n",
    "                              feed_dict={input_test: input_test_run})\n",
    "\n",
    "    risk_est = sess.run(risk,\n",
    "                        feed_dict={x_test: input_test_run,\n",
    "                               y_test: output_test_run})\n",
    "    return risk_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die euklidische Norm des Gradienten wird berechnet:\n",
    "def compute_grad_norm(a):\n",
    "    #Die Eiträge von a werden in l eingefügt,\n",
    "    #wobei l ein Vektor ist, dessen Norm sich messen läst:\n",
    "    l = []\n",
    "    for i in range(len(a)):\n",
    "        shape = a[i].shape\n",
    "        for k1 in range(shape[0]):\n",
    "            if len(shape) == 2:                \n",
    "                for k2 in range(shape[1]):\n",
    "                    l.append(a[i][k1][k2])\n",
    "            else:\n",
    "                l.append(a[i][k1])\n",
    "    \n",
    "    #Nun wird die Norm von l gemessen:\n",
    "    sess = tf.InteractiveSession()\n",
    "    grad_norm = sess.run(tf.norm(l, 'euclidean'))\n",
    "    sess.close()\n",
    "    \n",
    "    return grad_norm, l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die euklidische Norm der Differenz zweier Variablen-Vektoren\n",
    "#wird berechnet:\n",
    "def compute_change(tvs1, tvs2):\n",
    "    #In l werden die Differenzen der Einträge von tvs2 und tvs1 eigefügt,\n",
    "    #wobei l ein Vektor ist, dessen Norm sich messen läst:\n",
    "    l = []\n",
    "    for i in range(len(tvs1)):\n",
    "        shape = tvs1[i].shape\n",
    "        for k1 in range(shape[0]):\n",
    "            if len(shape) == 2:                \n",
    "                for k2 in range(shape[1]):\n",
    "                    l.append(tvs2[i][k1][k2] - tvs1[i][k1][k2])\n",
    "            else:\n",
    "                l.append(tvs2[i][k1] - tvs1[i][k1])\n",
    "    \n",
    "    #Nun wird die Norm von l gemessen:\n",
    "    sess = tf.InteractiveSession()\n",
    "    change_norm = sess.run(tf.norm(l, 'euclidean'))\n",
    "    sess.close()\n",
    "    \n",
    "    return change_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Der Winkel zwischen zwei aufeinanderfolgen Gradienten a1 und a\n",
    "#wird berechnet:\n",
    "def compute_angle(a1, a):\n",
    "    len_a1, l_a1 = compute_grad_norm(a1)\n",
    "    len_a, l_a = compute_grad_norm(a)\n",
    "    if len_a1 == 0 or len_a == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    prod = sess.run(tf.tensordot(l_a1, l_a, 1))\n",
    "    prod2 = prod / (len_a1 * len_a)\n",
    "    angle = sess.run(tf.math.acos(prod2)*180/m.pi)\n",
    "    sess.close()\n",
    "    #Wegen Rechenungenauigkeiten kann prod2 außerhalb der\n",
    "    #theoretisch möglichen Range liegen.\n",
    "    if prod2 > 1:\n",
    "        return 0.0\n",
    "    if prod2 < -1:\n",
    "        return 180.0\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Funktion train_minibatch orientiert sich an der gleichnamigen \n",
    "#Funktion von: http://deeplearnphysics.org/Blog/minibatch.html\n",
    "\n",
    "def train_minibatch(opt_type, learning_rate, batch_size, minibatch_size,\n",
    "                    test_size, ntests, stddev, distnr):\n",
    "    #Die Anzahl der Durchgänge wird berechnet:\n",
    "    nsteps = batch_size // minibatch_size\n",
    "    # Das Optimierungsverfahren wird festgelegt:\n",
    "    opt = opt_type(learning_rate)\n",
    "\n",
    "    # 0) Die trainierbaren Variablen werden abgerufen:\n",
    "    tvs = tf.trainable_variables()\n",
    "    # 1) accum_vars ist ein placeholder für die Akkumulation des Gradienten:\n",
    "    accum_vars = [tf.Variable(tv.initialized_value(), trainable=False)\n",
    "                  for tv in tvs]\n",
    "    # 2) Operation um die Werte von accum_vars auf 0 zu setzen:\n",
    "    zero_ops  = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n",
    "    # 3) Operation um den Gradienten der loss-function bezüglich \n",
    "    #    eines minibatch zu berechnen:\n",
    "    gvs = opt.compute_gradients(loss=loss_minibatch, var_list=tvs)\n",
    "    # 4) Operation um die Gradienten in accum_vars zu akkumulieren:\n",
    "    accum_ops = [accum_vars[i].assign_add(gv[0]) \n",
    "                 for i, gv in enumerate(gvs)]\n",
    "    #Anmerkung: Die verschiedenen gv[0] enthalten die \n",
    "    #Zahlenwerte des Gradienten.\n",
    "    # 5) Operation um den Gradienten gemäß opt auf die Parameter anzuwenden:\n",
    "    apply_ops = opt.apply_gradients([(accum_vars[i], tv) for i, tv \n",
    "                                     in enumerate(tf.trainable_variables())])\n",
    "    \n",
    "    # Array zum speichern der Risiko-Werte:\n",
    "    risk_log = np.zeros(ntests)\n",
    "    # Array zum speichern der L2-Norm Werte der Gradienten:\n",
    "    grad_norm_log = np.zeros(ntests)\n",
    "    # Array zum speichern der Werte der Winkel zwischen zwei\n",
    "    # aufeinanderfolgen Gradienten:\n",
    "    angle_log = np.zeros(ntests)\n",
    "    # Array zum speichern der L2-Norm der differenz der Parametervektoren:\n",
    "    change_log = np.zeros(ntests)\n",
    "    # Array zum speichern der Abstände von:\n",
    "    dist_log = np.zeros(ntests - distnr)\n",
    "    \n",
    "    # Die session wird gestartet:\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Startwerte für die Variblen werden erzeugt:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #input und output werden ausgeführt, um in feed_dict verwendbar zu sein:\n",
    "    input_learn_run, output_learn_run = create_data(batch_size, stddev)    \n",
    "    # a1 erhält einen Startwert, damit im ersten Durchgang kein\n",
    "    # Error auftritt.\n",
    "    sess.run(zero_ops)\n",
    "    a1 = sess.run(accum_vars)\n",
    "    print(\"Startwerte der Parameter:\\n\", sess.run(tvs),\"\\n\\n\")\n",
    "    \n",
    "    # j nimmt die Werte von 0 bis nsteps - (nsteps // ntests) an,\n",
    "    # da in den letzten (nsteps // ntests) - 1 Durchgängen \n",
    "    # keine Messungen mehr durchgeführt werden.\n",
    "    # Trainings Phase:\n",
    "    for j in range(nsteps - (nsteps // ntests) + 1):\n",
    "        sess.run(zero_ops)\n",
    "        sess.run(accum_ops,\n",
    "                  feed_dict={x: input_learn_run[j*minibatch_size:(j+1)\n",
    "                                                *minibatch_size],\n",
    "                             y: output_learn_run[j*minibatch_size:(j+1)\n",
    "                                                 *minibatch_size]})\n",
    "        \n",
    "        #Die if Abfrage sorgt dafür, dass nur in bestimmten Schritten das \n",
    "        #Risiko und die Gradientennorm geschätzt werden:\n",
    "        if j % (nsteps // ntests) == 0:\n",
    "            #Das Risiko wird geschätzt:\n",
    "            risk_log[j // (nsteps // ntests)] = estimate_risk(test_size,\n",
    "                                                              sess, j, stddev)\n",
    "            #Die euklidische Norm des Gradienten wird berechnet:\n",
    "            a = sess.run(accum_vars)\n",
    "            grad_norm_log[j // (nsteps // ntests)] = compute_grad_norm(a)[0]\n",
    "            tvs_before_update = sess.run(tvs)\n",
    "            #Der Winkel zwischen dem Gradienten und dem vorherigen\n",
    "            #Gradienten wird berechnet:\n",
    "            angle_log[j // (nsteps // ntests)] = compute_angle(a1, a)\n",
    "            #Sobald der Test der Nummer distnr durchgeführt wird, werden die \n",
    "            #aktuellen Parameter gespeichert:\n",
    "            if j // (nsteps // ntests) == (distnr - 1):\n",
    "                tvs_fix = sess.run(tvs)\n",
    "            #Ab dem Test der Nummer distnr wird der Abstand der aktuellen\n",
    "            #Parameter zu tvs_fix berechnet.\n",
    "            if j // (nsteps // ntests) > (distnr - 1):\n",
    "                dist_log[j // (nsteps // ntests) - distnr] = compute_change(\n",
    "                    tvs_fix, sess.run(tvs))            \n",
    "            \n",
    "            #Die Nummer des Tests wird ausgegeben, um den \n",
    "            #Simulationsfortschritt anzuzeigen:\n",
    "            print(j // (nsteps // ntests) + 1)\n",
    "        \n",
    "        #Die if Abfrage sorgt dafür, dass nur in bestimmten Schritten der\n",
    "        #Gradient für die Winkelberechnung im nächsten Durchgang\n",
    "        #gespeichert wird:\n",
    "        if (j+1) % (nsteps // ntests) == 0:\n",
    "            a1 = sess.run(accum_vars)\n",
    "        \n",
    "        sess.run(apply_ops)\n",
    "\n",
    "        if j % (nsteps // ntests) == 0:\n",
    "            tvs_after_update = sess.run(tvs)\n",
    "            change_log[j // (nsteps // ntests)] = compute_change(\n",
    "                tvs_before_update, tvs_after_update)\n",
    "\n",
    "    print(\"Gradient des letzten Durchgangs:\\n\", a, \"\\n\\n\")\n",
    "    tvs_run = sess.run(tvs)    \n",
    "    sess.close()\n",
    "    return risk_log, grad_norm_log, change_log, tvs_run, angle_log, dist_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Funktion train_minibatch_diminish orientiert sich an der Funktion \n",
    "#train_minibatch von: http://deeplearnphysics.org/Blog/minibatch.html\n",
    "\n",
    "def train_minibatch_diminish(opt_type, learning_rate, batch_size,\n",
    "                             minibatch_size, test_size, ntests,\n",
    "                             stddev, distnr):\n",
    "    #Die Anzahl der Durchgänge wird berechnet:\n",
    "    nsteps = batch_size // minibatch_size\n",
    "    # Das Optimierungsverfahren wird festgelegt:\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    opt = opt_type(lr)\n",
    "\n",
    "    # 0) Die trainierbaren Variablen werden abgerufen:\n",
    "    tvs = tf.trainable_variables()\n",
    "    # 1) accum_vars ist ein placeholder für die Akkumulation des Gradienten:\n",
    "    accum_vars = [tf.Variable(tv.initialized_value(), trainable=False)\n",
    "                  for tv in tvs]\n",
    "    # 2) Operation um die Werte von accum_vars auf 0 zu setzen:\n",
    "    zero_ops  = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n",
    "    # 3) Operation um den Gradienten der loss-function bezüglich \n",
    "    #    eines minibatch zu berechnen:\n",
    "    gvs = opt.compute_gradients(loss=loss_minibatch, var_list=tvs)\n",
    "    # 4) Operation um die Gradienten in accum_vars zu akkumulieren:\n",
    "    accum_ops = [accum_vars[i].assign_add(gv[0]) \n",
    "                 for i, gv in enumerate(gvs)]\n",
    "    #Anmerkung: Die verschiedenen gv[0] enthalten die \n",
    "    #Zahlenwerte des Gradienten.\n",
    "    # 5) Operation um den Gradienten gemäß opt auf die Parameter anzuwenden:\n",
    "    apply_ops = opt.apply_gradients([(accum_vars[i], tv) for i, tv \n",
    "                                     in enumerate(tf.trainable_variables())])\n",
    "    \n",
    "    # Array zum speichern der Risiko-Werte:\n",
    "    risk_log = np.zeros(ntests)\n",
    "    # Array zum speichern der L2-Norm Werte der Gradienten:\n",
    "    grad_norm_log = np.zeros(ntests)\n",
    "    # Array zum speichern der Werte der Winkel zwischen zwei\n",
    "    # aufeinanderfolgen Gradienten:\n",
    "    angle_log = np.zeros(ntests)\n",
    "    # Array zum speichern der L2-Norm der differenz der Parametervektoren:\n",
    "    change_log = np.zeros(ntests)\n",
    "    # Array zum speichern der Abstände von:\n",
    "    dist_log = np.zeros(ntests - distnr)\n",
    "    \n",
    "    # Die session wird gestartet:\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Startwerte für die Variblen werden erzeugt:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #input und output werden ausgeführt, um in feed_dict verwendbar zu sein:\n",
    "    input_learn_run, output_learn_run = create_data(batch_size, stddev)    \n",
    "    # a1 erhält einen Startwert, damit im ersten Durchgang kein\n",
    "    # Error auftritt.\n",
    "    sess.run(zero_ops)\n",
    "    a1 = sess.run(accum_vars)\n",
    "    print(\"Startwerte der Parameter:\\n\", sess.run(tvs),\"\\n\\n\")\n",
    "    \n",
    "    # j nimmt die Werte von 0 bis nsteps - (nsteps // ntests) an,\n",
    "    # da in den letzten (nsteps // ntests) - 1 Durchgängen \n",
    "    # keine Messungen mehr durchgeführt werden.\n",
    "    # Trainings Phase:\n",
    "    for j in range(nsteps - (nsteps // ntests) + 1):\n",
    "        sess.run(zero_ops)\n",
    "        sess.run(accum_ops,\n",
    "                  feed_dict={x: input_learn_run[j*minibatch_size:(j+1)\n",
    "                                                *minibatch_size],\n",
    "                             y: output_learn_run[j*minibatch_size:(j+1)\n",
    "                                                 *minibatch_size]})\n",
    "        \n",
    "        #Die if Abfrage sorgt dafür, dass nur in bestimmten Schritten das \n",
    "        #Risiko und die Gradientennorm geschätzt werden:\n",
    "        if j % (nsteps // ntests) == 0:\n",
    "            #Das Risiko wird geschätzt:\n",
    "            risk_log[j // (nsteps // ntests)] = estimate_risk(test_size,\n",
    "                                                              sess, j, stddev)\n",
    "            #Die euklidische Norm des Gradienten wird berechnet:\n",
    "            a = sess.run(accum_vars)\n",
    "            grad_norm_log[j // (nsteps // ntests)] = compute_grad_norm(a)[0]\n",
    "            tvs_before_update = sess.run(tvs)\n",
    "            #Der Winkel zwischen dem Gradienten und dem vorherigen\n",
    "            #Gradienten wird berechnet:\n",
    "            angle_log[j // (nsteps // ntests)] = compute_angle(a1, a)\n",
    "            #Sobald der Test der Nummer distnr durchgeführt wird, werden die \n",
    "            #aktuellen Parameter gespeichert:\n",
    "            if j // (nsteps // ntests) == (distnr - 1):\n",
    "                tvs_fix = sess.run(tvs)\n",
    "            #Ab dem Test der Nummer distnr wird der Abstand der aktuellen\n",
    "            #Parameter zu tvs_fix berechnet.\n",
    "            if j // (nsteps // ntests) > (distnr - 1):\n",
    "                dist_log[j // (nsteps // ntests) - distnr] = compute_change(\n",
    "                    tvs_fix, sess.run(tvs))            \n",
    "            \n",
    "            #Die Nummer des Tests wird ausgegeben, um den \n",
    "            #Simulationsfortschritt anzuzeigen:\n",
    "            print(j // (nsteps // ntests) + 1)\n",
    "        \n",
    "        #Die if Abfrage sorgt dafür, dass nur in bestimmten Schritten der\n",
    "        #Gradient für die Winkelberechnung im nächsten Durchgang\n",
    "        #gespeichert wird:\n",
    "        if (j+1) % (nsteps // ntests) == 0:\n",
    "            a1 = sess.run(accum_vars)\n",
    "        \n",
    "        sess.run(apply_ops,\n",
    "                feed_dict={lr: learning_rate/(j+1)})\n",
    "        \n",
    "        if j % (nsteps // ntests) == 0:\n",
    "            tvs_after_update = sess.run(tvs)\n",
    "            change_log[j // (nsteps // ntests)] = compute_change(\n",
    "                tvs_before_update, tvs_after_update)\n",
    "\n",
    "    print(\"Gradient des letzten Durchgangs:\\n\", a, \"\\n\\n\")\n",
    "    tvs_run = sess.run(tvs)    \n",
    "    sess.close()\n",
    "    return risk_log, grad_norm_log, change_log, tvs_run, angle_log, dist_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Funktion Simulation() orientiert sich an der Funktion \n",
    "#\"compare()\" von: http://deeplearnphysics.org/Blog/minibatch.html\n",
    "\n",
    "# Die folgenden Lernverfahren lassen sich beispielsweise\n",
    "# für opt_type einsetzen:\n",
    "# \"Stochastischer Gradientenabstieg\":  tf.train.GradientDescentOptimizer\n",
    "# \"Adagrad\":                           tf.train.AdagradOptimizer\n",
    "# \"Adam\":                              tf.train.AdamOptimizer\n",
    "# \"RMSProp\"                            tf.train.RMSPropOptimizer\n",
    "\n",
    "def Simulation(name, opt_type, learning_rate, batch_size, minibatch_size,\n",
    "               test_size, ntests, stddev, diminish, distnr=1):\n",
    "    # Minibatch-Training wird durchgeführt:\n",
    "    if diminish == False:\n",
    "        #In diesem Fall wird das Training mit konstanter\n",
    "        #Lernrate durchgeführt\n",
    "        [risk_log, grad_norm_log, change_log, tvs_run, angle_log,\n",
    "         dist_log] = train_minibatch(opt_type=opt_type,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     batch_size=batch_size,\n",
    "                                     minibatch_size=minibatch_size,\n",
    "                                     test_size=test_size,\n",
    "                                     ntests=ntests,\n",
    "                                     stddev=stddev,\n",
    "                                     distnr=distnr)\n",
    "    if diminish == True:\n",
    "        #In diesem Fall wird das Training mit abnehmender\n",
    "        #Lernrate durchgeführt\n",
    "        [risk_log, grad_norm_log, change_log, tvs_run, angle_log,\n",
    "         dist_log] = train_minibatch_diminish(opt_type=opt_type, \n",
    "                                              learning_rate=learning_rate,\n",
    "                                              batch_size=batch_size,\n",
    "                                              minibatch_size=minibatch_size,\n",
    "                                              test_size=test_size,\n",
    "                                              ntests=ntests,\n",
    "                                              stddev=stddev,\n",
    "                                              distnr=distnr)\n",
    "        \n",
    "    print(\"Endwerte der Parameter:\\n\", tvs_run, \"\\n\\n\")\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    print(\"Empirisches Risiko:\\n\", risk_log)\n",
    "    # Im Graph wird der natürliche Logarithmus des \n",
    "    # empirische Risikos angezeigt.\n",
    "    plt.plot(range(1, ntests+1), sess.run(tf.math.log(risk_log)),\n",
    "             linestyle=\"\", marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\"ln(Empirisches Risiko)\")\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()        \n",
    "\n",
    "    print(\"L2 Norm des Gradienten:\\n\", grad_norm_log)\n",
    "    # Im Graph wird der natürliche Logarithmus der Norm des \n",
    "    # Gradienten angezeigt.\n",
    "    plt.plot(range(1, ntests+1), sess.run(tf.math.log(grad_norm_log)),\n",
    "             linestyle=\"\", marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\"ln(L2 Norm des Gradienten)\")\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"L2 Norm der Differenz zweier aufeinanderfolgender\")\n",
    "    print(\"Parametervektoren:\\n\", change_log)\n",
    "    # Im Graph wird der natürliche Logarithmus der Norm der\n",
    "    # Differenz angezeigt.\n",
    "    plt.plot(range(1, ntests+1), sess.run(tf.math.log(change_log)),\n",
    "             linestyle=\"\", marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\n",
    "        \"ln(L2 Norm der Differenz zweier aufeinanderfolgenderParametervektoren)\")\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Der erste Wert von angle_log wird nicht ausgegeben, da im\n",
    "    # ersten Durchgang noch kein Winkel gemessen werden konnte.\n",
    "    print(\"Winkel zwischen zwei aufeinanderfolgenden Gradienten:\\n\",\n",
    "          angle_log[1:test_size])\n",
    "    plt.plot(range(1, ntests), angle_log[1:test_size], linestyle=\"\",\n",
    "             marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\"Winkel zwischen zwei aufeinanderfolgenden Gradienten\")\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    " \n",
    "    print(\"Distanz zwischen Parametern und Parametern bei Messnumer \",\n",
    "          distnr, \":\\n\", dist_log)\n",
    "    plt.plot(range(distnr+1, ntests+1), dist_log, linestyle=\"\",\n",
    "             marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\"Distanz zwischen Parametern und \\n Parametern bei Messnumer \"\n",
    "               + str(distnr))\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'value' to a tensor and failed. Error: None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 208\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    523\u001b[0m               observed = ops.internal_convert_to_tensor(\n\u001b[1;32m--> 524\u001b[1;33m                   values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[0;32m    525\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 208\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0711f1764f85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mstddev\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mdiminish\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             distnr         = 50)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-c9ed2a99ed9f>\u001b[0m in \u001b[0;36mSimulation\u001b[1;34m(name, opt_type, learning_rate, batch_size, minibatch_size, test_size, ntests, stddev, diminish, distnr)\u001b[0m\n\u001b[0;32m     23\u001b[0m                                      \u001b[0mntests\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntests\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                                      \u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                                      distnr=distnr)\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdiminish\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#In diesem Fall wird das Training mit abnehmender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-13ad90d6d7de>\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(opt_type, learning_rate, batch_size, minibatch_size, test_size, ntests, stddev, distnr)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# 4) Operation um die Gradienten in accum_vars zu akkumulieren:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     accum_ops = [accum_vars[i].assign_add(gv[0]) \n\u001b[1;32m---> 23\u001b[1;33m                  for i, gv in enumerate(gvs)]\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m#Anmerkung: Die verschiedenen gv[0] enthalten die\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#Zahlenwerte des Gradienten.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-13ad90d6d7de>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# 4) Operation um die Gradienten in accum_vars zu akkumulieren:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     accum_ops = [accum_vars[i].assign_add(gv[0]) \n\u001b[1;32m---> 23\u001b[1;33m                  for i, gv in enumerate(gvs)]\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m#Anmerkung: Die verschiedenen gv[0] enthalten die\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#Zahlenwerte des Gradienten.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36massign_add\u001b[1;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \"\"\"\n\u001b[0;32m   1739\u001b[0m     assign = state_ops.assign_add(\n\u001b[1;32m-> 1740\u001b[1;33m         self._variable, delta, use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m   1741\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1742\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[1;34m(ref, value, use_locking, name)\u001b[0m\n\u001b[0;32m    186\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_ref_dtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     return gen_state_ops.assign_add(\n\u001b[1;32m--> 188\u001b[1;33m         ref, value, use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m    189\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[1;34m(ref, value, use_locking, name)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0muse_locking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_locking\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m--> 108\u001b[1;33m         \"AssignAdd\", ref=ref, value=value, use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    526\u001b[0m               raise ValueError(\n\u001b[0;32m    527\u001b[0m                   \u001b[1;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m                   (input_name, err))\n\u001b[0m\u001b[0;32m    529\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[0;32m    530\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[1;31mValueError\u001b[0m: Tried to convert 'value' to a tensor and failed. Error: None values not supported."
     ]
    }
   ],
   "source": [
    "Simulation( name           = \"Adagrad\",\n",
    "            opt_type       = tf.train.AdagradOptimizer,\n",
    "            learning_rate  = 0.1,\n",
    "            batch_size     = 1000000,\n",
    "            minibatch_size = 100,\n",
    "            test_size      = 10000,\n",
    "            ntests         = 100,\n",
    "            stddev         = 0,\n",
    "            diminish       = False,\n",
    "            distnr         = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
