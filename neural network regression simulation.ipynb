{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.12.0\n",
      "Numpy version: 1.15.4\n"
     ]
    }
   ],
   "source": [
    "print (\"TensorFlow version: \" + tf.__version__)\n",
    "print (\"Numpy version: \" + np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nfeatures ist die Dimension des Inputs.\n",
    "nfeatures = 3\n",
    "\n",
    "#Durch den festgelegten seed werden immer die gleichen Daten erzeugt.\n",
    "#Wählt man als batch_size einmal die Zahl n und ein anderes mal die Zahl n+1,\n",
    "#So gleichen sich jeweils die ersten n Daten.\n",
    "def create_data(batch_size, stddev):\n",
    "    #Parameter für die Dimension des Inputs:\n",
    "    \n",
    "    #Hier werden die Daten generiert:\n",
    "    #Input:\n",
    "    input_learn = tf.random_uniform([batch_size, nfeatures], minval=0,\n",
    "                                    maxval=100, seed=3140, name=\"input\")\n",
    "\n",
    "    #Output:\n",
    "    factors = tf.constant([[2], [-5], [4]], dtype=\"float32\")\n",
    "    norm = tf.random_normal([batch_size, 1], mean=0, stddev=stddev, seed=8235)\n",
    "    output_learn3 = tf.linalg.matmul(input_learn, factors)\n",
    "    output_learn2 = tf.math.add(output_learn3,\n",
    "                                tf.constant(30.0, shape=[batch_size, 1]))\n",
    "    output_learn = tf.math.add(output_learn2, norm, name=\"output\")\n",
    "\n",
    "    \n",
    "    #Die Daten werden ausgeführt:\n",
    "    sess = tf.InteractiveSession()\n",
    "    input_learn_run = sess.run(input_learn)\n",
    "    output_learn_run = sess.run(output_learn,\n",
    "                              feed_dict={input_learn: input_learn_run})\n",
    "    sess.close()\n",
    "    \n",
    "    return input_learn_run, output_learn_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholder für Input und Output werden definiert:\n",
    "x = tf.placeholder(tf.float32, [None, nfeatures])\n",
    "y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier werden alle Parameter definiert:\n",
    "tf.set_random_seed(1735)\n",
    "\n",
    "# Gewichte zwischen dem Input und hidden-layer nr. 1 mit 10 Knoten:\n",
    "W1 = tf.Variable(tf.random_uniform([nfeatures, 10], minval=-0.6794,\n",
    "                                   maxval=0.6794), name='W1')\n",
    "#Bias von hidden-layer nr. 1:\n",
    "b1 = tf.Variable(tf.random_normal([10], mean=0, stddev=0), name='b1')\n",
    "# Gewichte zwischen hidden-layer nr.1 und hidden-layer nr. 2 mit 50 Knoten:\n",
    "W2 = tf.Variable(tf.random_uniform([10, 10], minval=-0.5477,\n",
    "                                   maxval=0.5477), name='W2')\n",
    "#Bias von hidden-layer nr. 2:\n",
    "b2 = tf.Variable(tf.random_normal([10], mean=0, stddev=0), name='b2')\n",
    "# Gewichte zwischen hidden-layer nr.2 und hidden-layer nr. 3 mit 50 Knoten:\n",
    "W3 = tf.Variable(tf.random_uniform([10, 10], minval=-0.5477,\n",
    "                                   maxval=0.5477), name='W3')\n",
    "#Bias von hidden-layer nr. 3:\n",
    "b3 = tf.Variable(tf.random_normal([10], mean=0, stddev=0), name='b3')\n",
    "# Gewichte zwischen hidden-layer nr.3 und dem Output:\n",
    "W4 = tf.Variable(tf.random_uniform([10, 1], minval=-0.7385,\n",
    "                                   maxval=0.7385), name='W4')\n",
    "#Bias des Outputs:\n",
    "b4 = tf.Variable(tf.random_normal([1], mean=0, stddev=0), name='b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier werden alle Zustandsvariablen berechnet:\n",
    "# Berechne die Werte von hidden-layer nr.1:\n",
    "hidden_1_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_1_out = tf.nn.relu(hidden_1_out)\n",
    "# Berechne die Werte von hidden-layer nr.2:\n",
    "hidden_2_out = tf.add(tf.matmul(hidden_1_out, W2), b2)\n",
    "hidden_2_out = tf.nn.relu(hidden_2_out)\n",
    "# Berechne die Werte von hidden-layer nr.3:\n",
    "hidden_3_out = tf.add(tf.matmul(hidden_2_out, W3), b3)\n",
    "hidden_3_out = tf.nn.relu(hidden_3_out)\n",
    "# Berechne den Output: \n",
    "y_pred = tf.add(tf.matmul(hidden_3_out, W4), b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Das Format von loss_minibatch ist für die Anwandung in der\n",
    "#Funktion train_minibatch() zugeschnitten.\n",
    "loss_minibatch = tf.reduce_mean(tf.squared_difference(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier wird das Risiko empirisch bestimmt:\n",
    "x_test = tf.placeholder(tf.float32, [None, nfeatures])\n",
    "y_test = tf.placeholder(tf.float32, [None, 1])\n",
    "# Berechne die Werte von hidden-layer nr.1:\n",
    "hidden_1_out_test = tf.add(tf.matmul(x_test, W1), b1)\n",
    "hidden_1_out_test = tf.nn.relu(hidden_1_out_test)\n",
    "# Berechne die Werte von hidden-layer nr.2:\n",
    "hidden_2_out_test = tf.add(tf.matmul(hidden_1_out_test, W2), b2)\n",
    "hidden_2_out_test = tf.nn.relu(hidden_2_out_test)\n",
    "# Berechne die Werte von hidden-layer nr.3:\n",
    "hidden_3_out_test = tf.add(tf.matmul(hidden_2_out_test, W3), b3)\n",
    "hidden_3_out_test = tf.nn.relu(hidden_3_out_test)\n",
    "# Berechne den Output:\n",
    "y_pred_test = tf.add(tf.matmul(hidden_3_out_test, W4), b4)\n",
    "\n",
    "#Berechne das empirische Risiko:\n",
    "risk = tf.reduce_mean(tf.squared_difference(y_test, y_pred_test))\n",
    "\n",
    "def estimate_risk(test_size, sess, j, stddev):\n",
    "    #j wird als seed verwendet, damit bei verschiedenen estimate_risk\n",
    "    #verschiedene Testdaten erzeugt werden\n",
    "    #Testdaten:\n",
    "    #Input:\n",
    "    input_test = tf.random_uniform([test_size, nfeatures], minval=0,\n",
    "                                   maxval=100, name=\"input_test\", seed=j)\n",
    "\n",
    "    #Output:\n",
    "    factors_test = tf.constant([[2], [-5], [4]], dtype=\"float32\")\n",
    "    norm_test = tf.random_normal([test_size, 1], mean=0, stddev=stddev,\n",
    "                                 seed=2*j)\n",
    "    output_test3 = tf.linalg.matmul(input_test, factors_test)\n",
    "    output_test2 = tf.math.add(output_test3,\n",
    "                               tf.constant(30.0, shape=[test_size, 1]))\n",
    "    output_test = tf.math.add(output_test2, norm_test, name=\"output_test\")\n",
    "\n",
    "    input_test_run = sess.run(input_test)\n",
    "    output_test_run = sess.run(output_test,\n",
    "                              feed_dict={input_test: input_test_run})\n",
    "\n",
    "    risk_est = sess.run(risk,\n",
    "                        feed_dict={x_test: input_test_run,\n",
    "                               y_test: output_test_run})\n",
    "    return risk_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die euklidische Norm des Gradienten wird berechnet:\n",
    "def compute_grad_norm(a):\n",
    "    #Die Eiträge von a werden in l eingefügt,\n",
    "    #wobei l ein Vektor ist, dessen Norm sich messen läst:\n",
    "    l = []\n",
    "    for i in range(len(a)):\n",
    "        shape = a[i].shape\n",
    "        for k1 in range(shape[0]):\n",
    "            if len(shape) == 2:                \n",
    "                for k2 in range(shape[1]):\n",
    "                    l.append(a[i][k1][k2])\n",
    "            else:\n",
    "                l.append(a[i][k1])\n",
    "    \n",
    "    #Nun wird die Norm von l gemessen:\n",
    "    sess = tf.InteractiveSession()\n",
    "    grad_norm = sess.run(tf.norm(l, 'euclidean'))\n",
    "    sess.close()\n",
    "    \n",
    "    return grad_norm, l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die euklidische Norm der Differenz zweier Variablen-Vektoren\n",
    "#wird berechnet:\n",
    "def compute_change(tvs1, tvs2):\n",
    "    #In l werden die Differenzen der Einträge von tvs2 und tvs1 eigefügt,\n",
    "    #wobei l ein Vektor ist, dessen Norm sich messen läst:\n",
    "    l = []\n",
    "    for i in range(len(tvs1)):\n",
    "        shape = tvs1[i].shape\n",
    "        for k1 in range(shape[0]):\n",
    "            if len(shape) == 2:                \n",
    "                for k2 in range(shape[1]):\n",
    "                    l.append(tvs2[i][k1][k2] - tvs1[i][k1][k2])\n",
    "            else:\n",
    "                l.append(tvs2[i][k1] - tvs1[i][k1])\n",
    "    \n",
    "    #Nun wird die Norm von l gemessen:\n",
    "    sess = tf.InteractiveSession()\n",
    "    change_norm = sess.run(tf.norm(l, 'euclidean'))\n",
    "    sess.close()\n",
    "    \n",
    "    return change_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Der Winkel zwischen zwei aufeinanderfolgen Gradienten a1 und a\n",
    "#wird berechnet:\n",
    "def compute_angle(a1, a):\n",
    "    len_a1, l_a1 = compute_grad_norm(a1)\n",
    "    len_a, l_a = compute_grad_norm(a)\n",
    "    if len_a1 == 0 or len_a == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    prod = sess.run(tf.tensordot(l_a1, l_a, 1))\n",
    "    prod2 = prod / (len_a1 * len_a)\n",
    "    angle = sess.run(tf.math.acos(prod2)*180/m.pi)\n",
    "    sess.close()\n",
    "    #Wegen Rechenungenauigkeiten kann prod2 außerhalb der\n",
    "    #theoretisch möglichen Range liegen.\n",
    "    if prod2 > 1:\n",
    "        return 0.0\n",
    "    if prod2 < -1:\n",
    "        return 180.0\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Funktion train_minibatch orientiert sich an der gleichnamigen \n",
    "#Funktion von: http://deeplearnphysics.org/Blog/minibatch.html\n",
    "\n",
    "def train_minibatch(opt_type, learning_rate, batch_size, minibatch_size,\n",
    "                    test_size, ntests, stddev, distnr):\n",
    "    #Die Anzahl der Durchgänge wird berechnet:\n",
    "    nsteps = batch_size // minibatch_size\n",
    "    # Das Optimierungsverfahren wird festgelegt:\n",
    "    opt = opt_type(learning_rate)\n",
    "\n",
    "    # 0) Die trainierbaren Variablen werden abgerufen:\n",
    "    tvs = tf.trainable_variables()\n",
    "    # 1) accum_vars ist ein placeholder für die Akkumulation des Gradienten:\n",
    "    accum_vars = [tf.Variable(tv.initialized_value(), trainable=False)\n",
    "                  for tv in tvs]\n",
    "    # 2) Operation um die Werte von accum_vars auf 0 zu setzen:\n",
    "    zero_ops  = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n",
    "    # 3) Operation um den Gradienten der loss-function bezüglich \n",
    "    #    eines minibatch zu berechnen:\n",
    "    gvs = opt.compute_gradients(loss=loss_minibatch, var_list=tvs)\n",
    "    # 4) Operation um die Gradienten in accum_vars zu akkumulieren:\n",
    "    accum_ops = [accum_vars[i].assign_add(gv[0]) \n",
    "                 for i, gv in enumerate(gvs)]\n",
    "    #Anmerkung: Die verschiedenen gv[0] enthalten die \n",
    "    #Zahlenwerte des Gradienten.\n",
    "    # 5) Operation um den Gradienten gemäß opt auf die Parameter anzuwenden:\n",
    "    apply_ops = opt.apply_gradients([(accum_vars[i], tv) for i, tv \n",
    "                                     in enumerate(tf.trainable_variables())])\n",
    "    \n",
    "    # Array zum speichern der Risiko-Werte:\n",
    "    risk_log = np.zeros(ntests)\n",
    "    # Array zum speichern der L2-Norm Werte der Gradienten:\n",
    "    grad_norm_log = np.zeros(ntests)\n",
    "    # Array zum speichern der Werte der Winkel zwischen zwei\n",
    "    # aufeinanderfolgen Gradienten:\n",
    "    angle_log = np.zeros(ntests)\n",
    "    # Array zum speichern der L2-Norm der differenz der Parametervektoren:\n",
    "    change_log = np.zeros(ntests)\n",
    "    # Array zum speichern der Abstände von:\n",
    "    dist_log = np.zeros(ntests - distnr)\n",
    "    \n",
    "    # Die session wird gestartet:\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Startwerte für die Variblen werden erzeugt:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #input und output werden ausgeführt, um in feed_dict verwendbar zu sein:\n",
    "    input_learn_run, output_learn_run = create_data(batch_size, stddev)    \n",
    "    # a1 erhält einen Startwert, damit im ersten Durchgang kein\n",
    "    # Error auftritt.\n",
    "    sess.run(zero_ops)\n",
    "    a1 = sess.run(accum_vars)\n",
    "    print(\"Startwerte der Parameter:\\n\", sess.run(tvs),\"\\n\\n\")\n",
    "    \n",
    "    # j nimmt die Werte von 0 bis nsteps - (nsteps // ntests) an,\n",
    "    # da in den letzten (nsteps // ntests) - 1 Durchgängen \n",
    "    # keine Messungen mehr durchgeführt werden.\n",
    "    # Trainings Phase:\n",
    "    for j in range(nsteps - (nsteps // ntests) + 1):\n",
    "        sess.run(zero_ops)\n",
    "        sess.run(accum_ops,\n",
    "                  feed_dict={x: input_learn_run[j*minibatch_size:(j+1)\n",
    "                                                *minibatch_size],\n",
    "                             y: output_learn_run[j*minibatch_size:(j+1)\n",
    "                                                 *minibatch_size]})\n",
    "        \n",
    "        #Die if Abfrage sorgt dafür, dass nur in bestimmten Schritten das \n",
    "        #Risiko und die Gradientennorm geschätzt werden:\n",
    "        if j % (nsteps // ntests) == 0:\n",
    "            #Das Risiko wird geschätzt:\n",
    "            risk_log[j // (nsteps // ntests)] = estimate_risk(test_size,\n",
    "                                                              sess, j, stddev)\n",
    "            #Die euklidische Norm des Gradienten wird berechnet:\n",
    "            a = sess.run(accum_vars)\n",
    "            grad_norm_log[j // (nsteps // ntests)] = compute_grad_norm(a)[0]\n",
    "            tvs_before_update = sess.run(tvs)\n",
    "            #Der Winkel zwischen dem Gradienten und dem vorherigen\n",
    "            #Gradienten wird berechnet:\n",
    "            angle_log[j // (nsteps // ntests)] = compute_angle(a1, a)\n",
    "            #Sobald der Test der Nummer distnr durchgeführt wird, werden die \n",
    "            #aktuellen Parameter gespeichert:\n",
    "            if j // (nsteps // ntests) == (distnr - 1):\n",
    "                tvs_fix = sess.run(tvs)\n",
    "            #Ab dem Test der Nummer distnr wird der Abstand der aktuellen\n",
    "            #Parameter zu tvs_fix berechnet.\n",
    "            if j // (nsteps // ntests) > (distnr - 1):\n",
    "                dist_log[j // (nsteps // ntests) - distnr] = compute_change(\n",
    "                    tvs_fix, sess.run(tvs))            \n",
    "            \n",
    "            #Die Nummer des Tests wird ausgegeben, um den \n",
    "            #Simulationsfortschritt anzuzeigen:\n",
    "            print(j // (nsteps // ntests) + 1)\n",
    "        \n",
    "        #Die if Abfrage sorgt dafür, dass nur in bestimmten Schritten der\n",
    "        #Gradient für die Winkelberechnung im nächsten Durchgang\n",
    "        #gespeichert wird:\n",
    "        if (j+1) % (nsteps // ntests) == 0:\n",
    "            a1 = sess.run(accum_vars)\n",
    "        \n",
    "        sess.run(apply_ops)\n",
    "\n",
    "        if j % (nsteps // ntests) == 0:\n",
    "            tvs_after_update = sess.run(tvs)\n",
    "            change_log[j // (nsteps // ntests)] = compute_change(\n",
    "                tvs_before_update, tvs_after_update)\n",
    "\n",
    "    print(\"Gradient des letzten Durchgangs:\\n\", a, \"\\n\\n\")\n",
    "    tvs_run = sess.run(tvs)    \n",
    "    sess.close()\n",
    "    return risk_log, grad_norm_log, change_log, tvs_run, angle_log, dist_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Funktion train_minibatch_diminish orientiert sich an der Funktion \n",
    "#train_minibatch von: http://deeplearnphysics.org/Blog/minibatch.html\n",
    "\n",
    "def train_minibatch_diminish(opt_type, learning_rate, batch_size,\n",
    "                             minibatch_size, test_size, ntests,\n",
    "                             stddev, distnr):\n",
    "    #Die Anzahl der Durchgänge wird berechnet:\n",
    "    nsteps = batch_size // minibatch_size\n",
    "    # Das Optimierungsverfahren wird festgelegt:\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    opt = opt_type(lr)\n",
    "\n",
    "    # 0) Die trainierbaren Variablen werden abgerufen:\n",
    "    tvs = tf.trainable_variables()\n",
    "    # 1) accum_vars ist ein placeholder für die Akkumulation des Gradienten:\n",
    "    accum_vars = [tf.Variable(tv.initialized_value(), trainable=False)\n",
    "                  for tv in tvs]\n",
    "    # 2) Operation um die Werte von accum_vars auf 0 zu setzen:\n",
    "    zero_ops  = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n",
    "    # 3) Operation um den Gradienten der loss-function bezüglich \n",
    "    #    eines minibatch zu berechnen:\n",
    "    gvs = opt.compute_gradients(loss=loss_minibatch, var_list=tvs)\n",
    "    # 4) Operation um die Gradienten in accum_vars zu akkumulieren:\n",
    "    accum_ops = [accum_vars[i].assign_add(gv[0]) \n",
    "                 for i, gv in enumerate(gvs)]\n",
    "    #Anmerkung: Die verschiedenen gv[0] enthalten die \n",
    "    #Zahlenwerte des Gradienten.\n",
    "    # 5) Operation um den Gradienten gemäß opt auf die Parameter anzuwenden:\n",
    "    apply_ops = opt.apply_gradients([(accum_vars[i], tv) for i, tv \n",
    "                                     in enumerate(tf.trainable_variables())])\n",
    "    \n",
    "    # Array zum speichern der Risiko-Werte:\n",
    "    risk_log = np.zeros(ntests)\n",
    "    # Array zum speichern der L2-Norm Werte der Gradienten:\n",
    "    grad_norm_log = np.zeros(ntests)\n",
    "    # Array zum speichern der Werte der Winkel zwischen zwei\n",
    "    # aufeinanderfolgen Gradienten:\n",
    "    angle_log = np.zeros(ntests)\n",
    "    # Array zum speichern der L2-Norm der differenz der Parametervektoren:\n",
    "    change_log = np.zeros(ntests)\n",
    "    # Array zum speichern der Abstände von:\n",
    "    dist_log = np.zeros(ntests - distnr)\n",
    "    \n",
    "    # Die session wird gestartet:\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Startwerte für die Variblen werden erzeugt:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #input und output werden ausgeführt, um in feed_dict verwendbar zu sein:\n",
    "    input_learn_run, output_learn_run = create_data(batch_size, stddev)    \n",
    "    # a1 erhält einen Startwert, damit im ersten Durchgang kein\n",
    "    # Error auftritt.\n",
    "    sess.run(zero_ops)\n",
    "    a1 = sess.run(accum_vars)\n",
    "    print(\"Startwerte der Parameter:\\n\", sess.run(tvs),\"\\n\\n\")\n",
    "    \n",
    "    # j nimmt die Werte von 0 bis nsteps - (nsteps // ntests) an,\n",
    "    # da in den letzten (nsteps // ntests) - 1 Durchgängen \n",
    "    # keine Messungen mehr durchgeführt werden.\n",
    "    # Trainings Phase:\n",
    "    for j in range(nsteps - (nsteps // ntests) + 1):\n",
    "        sess.run(zero_ops)\n",
    "        sess.run(accum_ops,\n",
    "                  feed_dict={x: input_learn_run[j*minibatch_size:(j+1)\n",
    "                                                *minibatch_size],\n",
    "                             y: output_learn_run[j*minibatch_size:(j+1)\n",
    "                                                 *minibatch_size]})\n",
    "        \n",
    "        #Die if Abfrage sorgt dafür, dass nur in bestimmten Schritten das \n",
    "        #Risiko und die Gradientennorm geschätzt werden:\n",
    "        if j % (nsteps // ntests) == 0:\n",
    "            #Das Risiko wird geschätzt:\n",
    "            risk_log[j // (nsteps // ntests)] = estimate_risk(test_size,\n",
    "                                                              sess, j, stddev)\n",
    "            #Die euklidische Norm des Gradienten wird berechnet:\n",
    "            a = sess.run(accum_vars)\n",
    "            grad_norm_log[j // (nsteps // ntests)] = compute_grad_norm(a)[0]\n",
    "            tvs_before_update = sess.run(tvs)\n",
    "            #Der Winkel zwischen dem Gradienten und dem vorherigen\n",
    "            #Gradienten wird berechnet:\n",
    "            angle_log[j // (nsteps // ntests)] = compute_angle(a1, a)\n",
    "            #Sobald der Test der Nummer distnr durchgeführt wird, werden die \n",
    "            #aktuellen Parameter gespeichert:\n",
    "            if j // (nsteps // ntests) == (distnr - 1):\n",
    "                tvs_fix = sess.run(tvs)\n",
    "            #Ab dem Test der Nummer distnr wird der Abstand der aktuellen\n",
    "            #Parameter zu tvs_fix berechnet.\n",
    "            if j // (nsteps // ntests) > (distnr - 1):\n",
    "                dist_log[j // (nsteps // ntests) - distnr] = compute_change(\n",
    "                    tvs_fix, sess.run(tvs))            \n",
    "            \n",
    "            #Die Nummer des Tests wird ausgegeben, um den \n",
    "            #Simulationsfortschritt anzuzeigen:\n",
    "            print(j // (nsteps // ntests) + 1)\n",
    "        \n",
    "        #Die if Abfrage sorgt dafür, dass nur in bestimmten Schritten der\n",
    "        #Gradient für die Winkelberechnung im nächsten Durchgang\n",
    "        #gespeichert wird:\n",
    "        if (j+1) % (nsteps // ntests) == 0:\n",
    "            a1 = sess.run(accum_vars)\n",
    "        \n",
    "        sess.run(apply_ops,\n",
    "                feed_dict={lr: learning_rate/(j+1)})\n",
    "        \n",
    "        if j % (nsteps // ntests) == 0:\n",
    "            tvs_after_update = sess.run(tvs)\n",
    "            change_log[j // (nsteps // ntests)] = compute_change(\n",
    "                tvs_before_update, tvs_after_update)\n",
    "\n",
    "    print(\"Gradient des letzten Durchgangs:\\n\", a, \"\\n\\n\")\n",
    "    tvs_run = sess.run(tvs)    \n",
    "    sess.close()\n",
    "    return risk_log, grad_norm_log, change_log, tvs_run, angle_log, dist_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Funktion Simulation() orientiert sich an der Funktion \n",
    "#\"compare()\" von: http://deeplearnphysics.org/Blog/minibatch.html\n",
    "\n",
    "# Die folgenden Lernverfahren lassen sich beispielsweise\n",
    "# für opt_type einsetzen:\n",
    "# \"Stochastischer Gradientenabstieg\":  tf.train.GradientDescentOptimizer\n",
    "# \"Adagrad\":                           tf.train.AdagradOptimizer\n",
    "# \"Adam\":                              tf.train.AdamOptimizer\n",
    "# \"RMSProp\"                            tf.train.RMSPropOptimizer\n",
    "\n",
    "def Simulation(name, opt_type, learning_rate, batch_size, minibatch_size,\n",
    "               test_size, ntests, stddev, diminish, distnr=1):\n",
    "    # Minibatch-Training wird durchgeführt:\n",
    "    if diminish == False:\n",
    "        #In diesem Fall wird das Training mit konstanter\n",
    "        #Lernrate durchgeführt\n",
    "        [risk_log, grad_norm_log, change_log, tvs_run, angle_log,\n",
    "         dist_log] = train_minibatch(opt_type=opt_type,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     batch_size=batch_size,\n",
    "                                     minibatch_size=minibatch_size,\n",
    "                                     test_size=test_size,\n",
    "                                     ntests=ntests,\n",
    "                                     stddev=stddev,\n",
    "                                     distnr=distnr)\n",
    "    if diminish == True:\n",
    "        #In diesem Fall wird das Training mit abnehmender\n",
    "        #Lernrate durchgeführt\n",
    "        [risk_log, grad_norm_log, change_log, tvs_run, angle_log,\n",
    "         dist_log] = train_minibatch_diminish(opt_type=opt_type, \n",
    "                                              learning_rate=learning_rate,\n",
    "                                              batch_size=batch_size,\n",
    "                                              minibatch_size=minibatch_size,\n",
    "                                              test_size=test_size,\n",
    "                                              ntests=ntests,\n",
    "                                              stddev=stddev,\n",
    "                                              distnr=distnr)\n",
    "        \n",
    "    print(\"Endwerte der Parameter:\\n\", tvs_run, \"\\n\\n\")\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    print(\"Empirisches Risiko:\\n\", risk_log)\n",
    "    # Im Graph wird der natürliche Logarithmus des \n",
    "    # empirische Risikos angezeigt.\n",
    "    plt.plot(range(1, ntests+1), sess.run(tf.math.log(risk_log)),\n",
    "             linestyle=\"\", marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\"ln(Empirisches Risiko)\")\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()        \n",
    "\n",
    "    print(\"L2 Norm des Gradienten:\\n\", grad_norm_log)\n",
    "    # Im Graph wird der natürliche Logarithmus der Norm des \n",
    "    # Gradienten angezeigt.\n",
    "    plt.plot(range(1, ntests+1), sess.run(tf.math.log(grad_norm_log)),\n",
    "             linestyle=\"\", marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\"ln(L2 Norm des Gradienten)\")\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"L2 Norm der Differenz zweier aufeinanderfolgender\")\n",
    "    print(\"Parametervektoren:\\n\", change_log)\n",
    "    # Im Graph wird der natürliche Logarithmus der Norm der\n",
    "    # Differenz angezeigt.\n",
    "    plt.plot(range(1, ntests+1), sess.run(tf.math.log(change_log)),\n",
    "             linestyle=\"\", marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\n",
    "        \"ln(L2 Norm der Differenz zweier aufeinanderfolgenderParametervektoren)\")\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Der erste Wert von angle_log wird nicht ausgegeben, da im\n",
    "    # ersten Durchgang noch kein Winkel gemessen werden konnte.\n",
    "    print(\"Winkel zwischen zwei aufeinanderfolgenden Gradienten:\\n\",\n",
    "          angle_log[1:test_size])\n",
    "    plt.plot(range(1, ntests), angle_log[1:test_size], linestyle=\"\",\n",
    "             marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\"Winkel zwischen zwei aufeinanderfolgenden Gradienten\")\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    " \n",
    "    print(\"Distanz zwischen Parametern und Parametern bei Messnumer \",\n",
    "          distnr, \":\\n\", dist_log)\n",
    "    plt.plot(range(distnr+1, ntests+1), dist_log, linestyle=\"\",\n",
    "             marker=\"o\", markersize=3)\n",
    "    plt.ylabel(\"Distanz zwischen Parametern und \\n Parametern bei Messnumer \"\n",
    "               + str(distnr))\n",
    "    plt.xlabel(\"Messnummer\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startwerte der Parameter:\n",
      " [array([[-0.6110125 , -0.18770587, -0.4823444 , -0.1428684 ,  0.6453    ,\n",
      "        -0.48833126,  0.1104008 ,  0.05433512, -0.60538983, -0.35003185],\n",
      "       [ 0.06745219,  0.49680835,  0.63196653,  0.05358058, -0.3616043 ,\n",
      "         0.33430475, -0.55532324, -0.22302872, -0.07072246, -0.24618885],\n",
      "       [-0.33270937,  0.01176584, -0.18765843,  0.30228132,  0.6189776 ,\n",
      "        -0.32064077,  0.22669858,  0.2811545 ,  0.05417603,  0.66704565]],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.05512249,  0.2613293 , -0.26416656,  0.07777822,  0.25300837,\n",
      "         0.4696167 ,  0.5241062 ,  0.00465918, -0.00619674, -0.4958543 ],\n",
      "       [ 0.16727376, -0.5453213 ,  0.14195162,  0.33532858, -0.16744721,\n",
      "        -0.12834394,  0.37315136, -0.19465065, -0.00213069,  0.11418235],\n",
      "       [ 0.2734235 ,  0.2142058 ,  0.373087  ,  0.52685565,  0.26478148,\n",
      "        -0.2017402 , -0.07293087, -0.47509804,  0.25101554, -0.12170607],\n",
      "       [ 0.05100214,  0.2593614 ,  0.17302293, -0.24797976, -0.16736218,\n",
      "         0.47180504,  0.03661126, -0.01478827,  0.31015033, -0.21208438],\n",
      "       [ 0.4265241 ,  0.25469583,  0.19576478, -0.07001433,  0.12552065,\n",
      "        -0.05079204,  0.37755406,  0.2789219 ,  0.0915935 ,  0.43194586],\n",
      "       [-0.47555912, -0.14189258, -0.383009  , -0.02894425, -0.2812408 ,\n",
      "        -0.12328625,  0.39205253,  0.52626246, -0.25926453,  0.2192967 ],\n",
      "       [-0.41107756,  0.22572821,  0.05160606,  0.10107613,  0.2848878 ,\n",
      "         0.45742625,  0.4949165 , -0.21082753, -0.22859645,  0.26856524],\n",
      "       [-0.25780892, -0.22847307,  0.15361899,  0.41194433, -0.03491551,\n",
      "         0.34832865,  0.17474514,  0.16322589, -0.5060828 , -0.1440481 ],\n",
      "       [ 0.47262365, -0.31646967, -0.50635636, -0.04114711,  0.05002904,\n",
      "        -0.20835066, -0.14866653, -0.12216049,  0.35574007, -0.41963303],\n",
      "       [ 0.45015258,  0.20846075, -0.4925511 , -0.2402693 , -0.36303794,\n",
      "         0.32373738, -0.45518756,  0.47202832,  0.33164853,  0.22893375]],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.10035646,  0.4879958 ,  0.22621632, -0.32447124, -0.2773146 ,\n",
      "        -0.08000946, -0.28456292,  0.19583243,  0.10975403, -0.40465957],\n",
      "       [-0.44118255, -0.36590028,  0.02224302,  0.26918548,  0.30407107,\n",
      "         0.4289863 ,  0.21307498,  0.21387595,  0.4973386 , -0.4845621 ],\n",
      "       [-0.19771647,  0.3740086 , -0.3519395 , -0.2441464 ,  0.2622137 ,\n",
      "        -0.28436288, -0.39293975,  0.3730871 ,  0.3793437 ,  0.53162926],\n",
      "       [-0.3227375 ,  0.02631629,  0.3753224 ,  0.44170785,  0.23207492,\n",
      "         0.168715  ,  0.408588  , -0.09531704,  0.4259687 ,  0.0487321 ],\n",
      "       [ 0.17863166,  0.19850546, -0.4251113 , -0.4801876 ,  0.2446379 ,\n",
      "        -0.36146167, -0.31389645,  0.1784991 , -0.5087045 , -0.00511986],\n",
      "       [ 0.21429932,  0.39850384,  0.14822096,  0.47804958, -0.39795864,\n",
      "         0.13633204, -0.26038635,  0.15718997,  0.45596915, -0.19507962],\n",
      "       [-0.06626129, -0.48308128,  0.521519  , -0.2124201 , -0.231504  ,\n",
      "        -0.22503066,  0.40963525,  0.38709372, -0.08695158,  0.10668409],\n",
      "       [ 0.07015604,  0.13402659, -0.3207947 , -0.24793524, -0.09555992,\n",
      "         0.45929605,  0.46410543,  0.19474834,  0.5185309 ,  0.03610969],\n",
      "       [ 0.22973895, -0.38529313,  0.4309926 , -0.00302714,  0.28999788,\n",
      "        -0.5067951 , -0.17121604,  0.44302756,  0.15041918, -0.3464142 ],\n",
      "       [ 0.18173611, -0.15289477,  0.1672644 ,  0.09904999,  0.07669985,\n",
      "        -0.35836896, -0.06685764, -0.28187674, -0.10789835,  0.09990585]],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.04287004],\n",
      "       [ 0.1514051 ],\n",
      "       [-0.07249683],\n",
      "       [-0.5545145 ],\n",
      "       [-0.06202352],\n",
      "       [ 0.5481634 ],\n",
      "       [ 0.63204575],\n",
      "       [ 0.03284502],\n",
      "       [ 0.6010983 ],\n",
      "       [-0.50882167]], dtype=float32), array([0.], dtype=float32)] \n",
      "\n",
      "\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0711f1764f85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mstddev\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mdiminish\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             distnr         = 50)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-c9ed2a99ed9f>\u001b[0m in \u001b[0;36mSimulation\u001b[1;34m(name, opt_type, learning_rate, batch_size, minibatch_size, test_size, ntests, stddev, diminish, distnr)\u001b[0m\n\u001b[0;32m     23\u001b[0m                                      \u001b[0mntests\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntests\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                                      \u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                                      distnr=distnr)\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdiminish\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#In diesem Fall wird das Training mit abnehmender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-13ad90d6d7de>\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(opt_type, learning_rate, batch_size, minibatch_size, test_size, ntests, stddev, distnr)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;31m#Die euklidische Norm des Gradienten wird berechnet:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccum_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mgrad_norm_log\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnsteps\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mntests\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mtvs_before_update\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtvs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;31m#Der Winkel zwischen dem Gradienten und dem vorherigen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-80d9a6e720af>\u001b[0m in \u001b[0;36mcompute_grad_norm\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#Nun wird die Norm von l gemessen:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Simulation( name           = \"Adagrad\",\n",
    "            opt_type       = tf.train.AdagradOptimizer,\n",
    "            learning_rate  = 0.1,\n",
    "            batch_size     = 1000000,\n",
    "            minibatch_size = 100,\n",
    "            test_size      = 10000,\n",
    "            ntests         = 100,\n",
    "            stddev         = 0,\n",
    "            diminish       = False,\n",
    "            distnr         = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
